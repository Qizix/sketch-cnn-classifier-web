{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(); sns.set_style('dark')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cairosvg\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Динамічне виділення пам'яті\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Виведення конфігурації\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = (config['model']['input_size'], config['model']['input_size'])\n",
    "img_size = config['model']['input_size']\n",
    "BATCH_SIZE = config['model']['batch_size']\n",
    "NUM_CLASSES = config['model']['output_size']\n",
    "PNG_DIR = config['data']['png_path']\n",
    "TFREC_TRAIN_DIR = config['data']['precessed_train_path']\n",
    "TFREC_VALID_DIR = config['data']['precessed_valid_path']\n",
    "MODEL_FINAL_DIR = config['training']['final_model_dir']\n",
    "MODEL_CHECK_DIR = config['training']['checkpoint_dir']\n",
    "class_names = sorted(os.listdir(PNG_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "\n",
    "# Функція для парсингу TFRecord\n",
    "def parse_tfrecord_onehot(example_proto, num_classes):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([num_classes], tf.float32),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)  # Декодуємо JPEG\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Конвертуємо в float32\n",
    "    label = example['label']\n",
    "    return image, label\n",
    "\n",
    "# Завантажуємо TFRecord файли\n",
    "def load_tfrecord_onehot_dataset(tfrecord_files, num_classes, batch_size=BATCH_SIZE):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_onehot(x, num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def augment_image(image):\n",
    "    # Випадкове горизонтальне віддзеркалення\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Функція для застосування аугментації під час завантаження даних\n",
    "def augment_image_during_training(image, label):\n",
    "    image = augment_image(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Завантажуємо тренувальний датасет\n",
    "train_tfrecord_files = [os.path.join(TFREC_TRAIN_DIR, f) for f in os.listdir(TFREC_TRAIN_DIR) if f.endswith('.tfrecord')]\n",
    "train_dataset = load_tfrecord_onehot_dataset(train_tfrecord_files, NUM_CLASSES, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Застосовуємо аугментацію до тренувального датасету\n",
    "train_dataset = train_dataset.map(\n",
    "    augment_image_during_training,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Додаємо батчування та prefetch для оптимізації\n",
    "train_dataset = train_dataset.repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Розраховуємо загальну кількість тренувальних зразків\n",
    "total_train_samples = sum(1 for _ in tf.data.TFRecordDataset(train_tfrecord_files))\n",
    "\n",
    "\n",
    "# Завантажуємо валідаційний датасет\n",
    "val_tfrecord_files = [os.path.join(TFREC_VALID_DIR, f) for f in os.listdir(TFREC_VALID_DIR) if f.endswith('.tfrecord')]\n",
    "val_dataset = load_tfrecord_onehot_dataset(val_tfrecord_files, NUM_CLASSES, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Додаємо батчування та prefetch для валідаційного датасету\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Розраховуємо загальну кількість валідаційних зразків\n",
    "total_val_samples = sum(1 for _ in tf.data.TFRecordDataset(val_tfrecord_files))\n",
    "\n",
    "steps_per_epoch = (total_train_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "validation_steps = (total_val_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(dataset, num_images=4, is_onehot=False):\n",
    "    # Беремо один батч з даними\n",
    "    for images, labels in dataset.take(1):\n",
    "        images = images.numpy()  # Конвертуємо тензори в NumPy\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # Перевіряємо, чи є дані в батчі\n",
    "        batch_size = images.shape[0]\n",
    "        if batch_size == 0:\n",
    "            raise ValueError(\"Батч порожній, перевірте ваш датасет.\")\n",
    "        \n",
    "        # Візуалізуємо тільки до num_images зображень\n",
    "        num_images = min(num_images, batch_size)\n",
    "\n",
    "        # Створюємо сітку для візуалізації\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(2, 2, i + 1)\n",
    "            \n",
    "            # Отримуємо одне зображення\n",
    "            image = images[i]\n",
    "\n",
    "            # Відображення залежно від діапазону значень\n",
    "            if image.max() > 1.0:\n",
    "                plt.imshow(image.astype('uint8'))\n",
    "            else:\n",
    "                plt.imshow(image)\n",
    "            \n",
    "            # Отримуємо мітку\n",
    "            if is_onehot:\n",
    "                label = np.argmax(labels[i])\n",
    "            else:\n",
    "                label = labels[i]\n",
    "            \n",
    "            plt.title(f\"Label: {label}\")\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Виклик функції з урахуванням батчів\n",
    "visualize_images(train_dataset, is_onehot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2, VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Функція для створення моделі\n",
    "\n",
    "model_name = \"MobileNetV2\"\n",
    "EPOCHS = 10\n",
    "LAYERS_TO_UNFREEZE1 = 6 # Кількість шарів, які не тренуватимуться\n",
    "LAYERS_TO_UNFREEZE2 = 12  # Кількість шарів, які не тренуватимуться\n",
    "\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  # 1024 for MobileNetV2, for VGG19 needed like 4096\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)  # num_classes should match your dataset\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6),\n",
    "    ModelCheckpoint(MODEL_CHECK_DIR + model_name + 'base.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_stage_1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    validation_steps=validation_steps \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[-LAYERS_TO_UNFREEZE1:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model to apply the changes\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with a very low learning rate\n",
    "history_stage_2 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint(MODEL_CHECK_DIR + model_name + 'fine.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "for layer in model.layers[-LAYERS_TO_UNFREEZE2:]:  # Unfreeze last 6 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model to apply the changes\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with a very low learning rate\n",
    "history_stage_3 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=steps_per_epoch,  # Кількість кроків на епоху\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Збереження моделі\n",
    "model.save(MODEL_FINAL_DIR + model_name '.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
